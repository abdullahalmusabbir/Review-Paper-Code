# -*- coding: utf-8 -*-
"""12.Breast Cancer Classification using Machine Learning Techniques with Explainable AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AqBNlbYG42upaLCXQqQ3YBYR3xQaEJ0x
"""

pip install ucimlrepo

from ucimlrepo import fetch_ucirepo

# fetch dataset
breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17)

# data (as pandas dataframes)
X = breast_cancer_wisconsin_diagnostic.data.features
y = breast_cancer_wisconsin_diagnostic.data.targets

# metadata
print(breast_cancer_wisconsin_diagnostic.metadata)

# variable information
print(breast_cancer_wisconsin_diagnostic.variables)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from ucimlrepo import fetch_ucirepo

# Models
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier

# Preprocessing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# XAI
import shap
import lime
import lime.lime_tabular

dataset = fetch_ucirepo(id=17)
X = dataset.data.features
y = dataset.data.targets

print("Shape of Features:", X.shape)
print("Shape of Target:", y.shape)

if y.dtypes[0] == "object":
    le = LabelEncoder()
    y = le.fit_transform(y.values.ravel())

print("Encoded Target Sample:", y[:10])

numeric_cols = X.select_dtypes(include=np.number).columns

plt.figure(figsize=(15,6))
X[numeric_cols].boxplot(rot=90)
plt.title("Outlier Detection (Before Capping)")
plt.show()

def cap_outliers(df):
    capped_df = df.copy()
    for col in df.select_dtypes(include=np.number).columns:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower = Q1 - 1.5 * IQR
        upper = Q3 + 1.5 * IQR
        capped_df[col] = np.where(df[col] < lower, lower,
                           np.where(df[col] > upper, upper, df[col]))
    return capped_df

X_capped = cap_outliers(X)

plt.figure(figsize=(15,6))
X_capped[numeric_cols].boxplot(rot=90)
plt.title("Outlier Detection (After Capping)")
plt.show()

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_capped)

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

models = {
    "Logistic Regression": LogisticRegression(max_iter=500, random_state=42),
    "SVM": SVC(kernel="rbf", probability=True, random_state=42),
    "Gradient Boosting": GradientBoostingClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(random_state=42),
    "KNN": KNeighborsClassifier(n_neighbors=5),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric="logloss", random_state=42),
    "Decision Tree": DecisionTreeClassifier(random_state=42)
}

results = {}

for name, model in models.items():
    print("="*40)
    print(f"Training {name}")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    acc = accuracy_score(y_test, y_pred)
    results[name] = acc

    print(f"{name} Accuracy: {acc:.4f}")
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
    print("Classification Report:\n", classification_report(y_test, y_pred))

plt.figure(figsize=(8,5))
sns.barplot(x=list(results.keys()), y=list(results.values()))
plt.title("Model Accuracy Comparison")
plt.ylabel("Accuracy")
plt.xticks(rotation=30)
plt.ylim(0.85, 1.0)
plt.show()

svm_model = models["SVM"]

# ---- SHAP ----
explainer = shap.KernelExplainer(svm_model.predict_proba, X_train[:50])
shap_values = explainer.shap_values(X_test[:20])

shap.summary_plot(shap_values, X_test[:20], feature_names=dataset.data.features.columns)

# ---- LIME ----
lime_explainer = lime.lime_tabular.LimeTabularExplainer(
    training_data=np.array(X_train),
    feature_names=dataset.data.features.columns,
    class_names=["Benign", "Malignant"],
    mode="classification"
)

i = 5  # pick a sample
exp = lime_explainer.explain_instance(X_test[i], svm_model.predict_proba, num_features=10)
exp.show_in_notebook(show_table=True)